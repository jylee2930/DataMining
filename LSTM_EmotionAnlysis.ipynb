{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNyJykIhYi0XwNUbXasV7ot",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jylee2930/DataMining/blob/main/LSTM_EmotionAnlysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Uv5TduSaMR_"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import urllib.request\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 한글 형태소 다운받기"
      ],
      "metadata": {
        "id": "4sA10mATmq_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git"
      ],
      "metadata": {
        "id": "X_r_5zHSdJgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd Mecab-ko-for-Google-Colab/"
      ],
      "metadata": {
        "id": "I_QvbO1hdM5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! bash install_mecab-ko_on_colab_light_220429.sh"
      ],
      "metadata": {
        "id": "KqWSmY27dPN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kkma, Komoran, Okt, Mecab 형태소\n",
        "import konlpy\n",
        "from konlpy.tag import Kkma, Komoran, Okt, Mecab\n",
        "\n",
        "kkma = Kkma()\n",
        "komoran = Komoran()\n",
        "okt = Okt()\n",
        "mecab = Mecab()"
      ],
      "metadata": {
        "id": "_Gex4_k3dkmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. 데이터 업로드"
      ],
      "metadata": {
        "id": "S4x9C1Jemym_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")\n",
        "train_data = pd.read_table('ratings_train.txt')\n",
        "test_data = pd.read_table('ratings_test.txt')"
      ],
      "metadata": {
        "id": "4dnLT43yeM7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 현재 작업 디렉토리 확인\n",
        "import os\n",
        "print(os.getcwd())  # /content 출력됨\n",
        "\n",
        "# 파일 존재 확인\n",
        "print(os.path.exists('ratings_train.txt'))  # True\n",
        "print(os.path.exists('ratings_test.txt'))   # True\n",
        "\n",
        "# 파일 목록 보기\n",
        "!ls -l ratings*.txt"
      ],
      "metadata": {
        "id": "zRbE-Dg7I_eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련용 리뷰 개수 :',len(train_data))"
      ],
      "metadata": {
        "id": "yuL8A0BlfBwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('검사용 리뷰 개수 :',len(test_data))"
      ],
      "metadata": {
        "id": "SrsxDGD1fEyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[:5]"
      ],
      "metadata": {
        "id": "AE8Lg5JxfcS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data[:5]"
      ],
      "metadata": {
        "id": "8LKktU8Gfhna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(train_data)"
      ],
      "metadata": {
        "id": "hz4UaU6yFnHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##결측치 확인\n",
        "train_data['document'].isnull().sum()"
      ],
      "metadata": {
        "id": "0MqD_LzJAdMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['label'].isnull().sum()"
      ],
      "metadata": {
        "id": "AQ4DUpHPB2To"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##결측치 행 제거\n",
        "train_data.dropna()"
      ],
      "metadata": {
        "id": "imqXJbLICAzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data.isnull().values.any()) # Null 값이 존재하는지 확인"
      ],
      "metadata": {
        "id": "8FiUaI0WErZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.dropna(how='any')"
      ],
      "metadata": {
        "id": "4FyiX1B3E0Ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data.isnull().values.any()) # Null 값이 존재하는지 확인"
      ],
      "metadata": {
        "id": "BmF-rBUNE7VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['document'].isnull().sum()"
      ],
      "metadata": {
        "id": "LEW2VdzRE6Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gN5EzsPAFdq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# document 열과 label 열의 중복을 제외한 값의 개수\n",
        "train_data['document'].nunique(), train_data['label'].nunique()"
      ],
      "metadata": {
        "id": "wsDD_Plkfzry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
        "print(train_data.isnull().values.any()) # Null 값이 존재하는지 확인"
      ],
      "metadata": {
        "id": "pKzqM8RrgFYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 한글과 공백을 제외하고 모두 제거\n",
        "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
        "train_data[:5]"
      ],
      "metadata": {
        "id": "txZgq26QgqBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.drop_duplicates(subset = ['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
        "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 한글과 공배 외 모두 제거\n",
        "test_data['document'] = test_data['document'].str.replace('^ +', \"\") # 공백은 empty 값으로 변경\n",
        "test_data['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
        "test_data = test_data.dropna(how='any') # Null 값 제거\n",
        "print('전처리 후 테스트용 샘플의 개수 :',len(test_data))"
      ],
      "metadata": {
        "id": "OcJcAucDhGAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 토큰화\n",
        "1) 불용어 사전 만들기      \n",
        "2) 형태소 분석기 사용"
      ],
      "metadata": {
        "id": "Ahtvuvnkm75j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
      ],
      "metadata": {
        "id": "FgjQ5a9jhzyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = []\n",
        "for sentence in tqdm(train_data['document']):\n",
        "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
        "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
        "    X_train.append(stopwords_removed_sentence)"
      ],
      "metadata": {
        "id": "TU1EVWQMibTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = []\n",
        "for sentence in tqdm(test_data['document']):\n",
        "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
        "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
        "    X_test.append(stopwords_removed_sentence)"
      ],
      "metadata": {
        "id": "47kTcnVOihiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 정수 인코딩    \n",
        "1) 빈도수가 3 이하인 단어 제외    \n",
        "2) 단어 집합의 최대 크기 구하기     \n",
        "3) 정수 인코딩\n"
      ],
      "metadata": {
        "id": "A3ON-_yrnRJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "print(tokenizer.word_index)"
      ],
      "metadata": {
        "id": "qKniivkNo-kW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 3\n",
        "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "for key, value in tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value"
      ],
      "metadata": {
        "id": "uSjHiMbFo1oK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 단어 개수 중 빈도수 2이하인 단어는 제거.\n",
        "# 0번 패딩 토큰을 고려하여 + 1\n",
        "vocab_size = total_cnt - rare_cnt + 1\n",
        "print('단어 집합의 크기 :',vocab_size)"
      ],
      "metadata": {
        "id": "OjPjQGHUpH_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(vocab_size)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)"
      ],
      "metadata": {
        "id": "esKwv2dEpKWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = np.array(train_data['label'])\n",
        "y_test = np.array(test_data['label'])"
      ],
      "metadata": {
        "id": "wQ5ZXNhIpZZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. 빈도수 작은 단으만으로 구성된 빈 샘플 제거"
      ],
      "metadata": {
        "id": "ELTPu0GvpmCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]"
      ],
      "metadata": {
        "id": "wjz7S4ChpuTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = [sentence for index, sentence in enumerate(X_train) if index not in drop_train]\n",
        "y_train = np.delete(y_train, drop_train, axis=0)\n",
        "print(len(X_train))\n",
        "print(len(y_train))"
      ],
      "metadata": {
        "id": "SJd-pWE2pyXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. 패딩"
      ],
      "metadata": {
        "id": "k1DAb3vhp1lR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('리뷰의 최대 길이 :',max(len(review) for review in X_train))\n",
        "print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))\n",
        "plt.hist([len(review) for review in X_train], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DVr3pgXvp0Lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def below_threshold_len(max_len, nested_list):\n",
        "  count = 0\n",
        "  for sentence in nested_list:\n",
        "    if(len(sentence) <= max_len):\n",
        "        count = count + 1\n",
        "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (count / len(nested_list))*100))"
      ],
      "metadata": {
        "id": "PwJV7Kf-qAa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 30\n",
        "below_threshold_len(max_len, X_train)"
      ],
      "metadata": {
        "id": "n1GkLdabqE_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)"
      ],
      "metadata": {
        "id": "IWOU2BkSqN35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델링"
      ],
      "metadata": {
        "id": "1q5OR6VhqQ0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "embedding_dim = 100\n",
        "hidden_units = 128\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(LSTM(hidden_units))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)"
      ],
      "metadata": {
        "id": "qoFWtqkBfmJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment_predict(new_sentence):\n",
        "  new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\n",
        "  new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n",
        "  new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
        "  encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
        "  pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
        "  score = float(model.predict(pad_new)[0][0]) # 예측\n",
        "  if(score > 0.5):\n",
        "    print(\"{:.2f}% 확률로 긍정 리뷰입니다.\\n\".format(score * 100))\n",
        "  else:\n",
        "    print(\"{:.2f}% 확률로 부정 리뷰입니다.\\n\".format((1 - score) * 100))"
      ],
      "metadata": {
        "id": "xn0FbLXaqam6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_predict('이 영화 개꿀잼 ㅋㅋㅋ')"
      ],
      "metadata": {
        "id": "QpKdOXn7qdQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_predict('아 더빙 진짜 짜증나네요..목소리')"
      ],
      "metadata": {
        "id": "xga58XlJH_Wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_predict('와 개쩐다 정말 세계관 최강자들의 영화다')"
      ],
      "metadata": {
        "id": "TfLeyhAgV5lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 샘플\n",
        "test_samples = [\n",
        "    \"이 영화 진짜 최고였어요! 강추합니다!\",\n",
        "    \"완전 시간 낭비 ㅉㅉ 돈 아까워\",\n",
        "    \"와 개쩐다 정말 세계관 최강자들의 영화다\",\n",
        "    \"이게 영화야? 차라리 유튜브 보는게 나음\",\n",
        "    \"배우들 연기가 너무 좋았고 스토리도 탄탄했어요\",\n",
        "    \"졸려서 중간에 잤음 ㅋㅋ\",\n",
        "    \"기대 안했는데 의외로 괜찮네요\",\n",
        "    \"역대급 쓰레기 영화\"\n",
        "]\n",
        "\n",
        "for sample in test_samples:\n",
        "    sentiment_predict(sample)\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "nqWVZdDCfvBh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}