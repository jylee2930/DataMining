{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOwAoz7DS9YUPrGKR/pRyRV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jylee2930/DataMining/blob/main/12%EC%A3%BC%EC%B0%A8_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "한국어 영화 리뷰 감정 분석 (NSMC Dataset)\n",
        "Naver Sentiment Movie Corpus를 사용한 긍정/부정 분류 모델\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# 1단계: 라이브러리 임포트\n",
        "# ============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "import urllib.request\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "u7ISW9J2edMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 2단계: 데이터 다운로드 및 로드\n",
        "# ============================================================================\n",
        "print(\"=\" * 50)\n",
        "print(\"데이터 다운로드 중...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "urllib.request.urlretrieve(\n",
        "    \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\",\n",
        "    filename=\"ratings_train.txt\"\n",
        ")\n",
        "urllib.request.urlretrieve(\n",
        "    \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\",\n",
        "    filename=\"ratings_test.txt\"\n",
        ")\n",
        "\n",
        "train_df = pd.read_table('ratings_train.txt')\n",
        "test_df = pd.read_table('ratings_test.txt')\n",
        "\n",
        "print(f\"\\n학습 데이터 크기: {train_df.shape}\")\n",
        "print(f\"테스트 데이터 크기: {test_df.shape}\")\n",
        "print(f\"\\n학습 데이터 샘플:\\n{train_df.head()}\")\n",
        "print(f\"\\n레이블 분포:\\n{train_df['label'].value_counts()}\")"
      ],
      "metadata": {
        "id": "sYmbxgx6eeXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 3단계: Mecab 형태소 분석기 설치 (Google Colab)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Mecab 형태소 분석기 설치 중...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Colab에서 실행\n",
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
        "%cd Mecab-ko-for-Google-Colab/\n",
        "!bash install_mecab-ko_on_colab_light_220429.sh\n",
        "%cd ..\n",
        "\n",
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n"
      ],
      "metadata": {
        "id": "bqqvPrewey0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 4단계: 데이터 전처리\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"데이터 전처리 시작...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 한글 불용어 정의\n",
        "STOP_WORDS = [\n",
        "    \"는\", \"을\", \"를\", \"이\", \"가\", \"의\", \"던\", \"고\", \"하\", \"다\",\n",
        "    \"은\", \"에\", \"들\", \"지\", \"게\", \"도\", \"와\", \"과\", \"으로\", \"만\"\n",
        "]\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"텍스트 전처리 함수\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return []\n",
        "    # 한글, 영어, 공백만 남기기\n",
        "    text = re.sub(\"[^A-Za-z가-힣ㄱ-ㅎㅏ-ㅣ ]\", \"\", text)\n",
        "    # 형태소 분석 및 불용어 제거\n",
        "    tokens = mecab.morphs(text)\n",
        "    tokens = [word for word in tokens if word not in STOP_WORDS and len(word) > 1]\n",
        "    return tokens\n",
        "\n",
        "# 학습 데이터 전처리\n",
        "print(\"학습 데이터 전처리 중...\")\n",
        "train_df['document'] = train_df['document'].astype(str)\n",
        "train_df = train_df.dropna(subset=['document'])\n",
        "train_texts = train_df['document'].apply(preprocess_text)\n",
        "train_labels = train_df['label'].values\n",
        "\n",
        "# 테스트 데이터 전처리\n",
        "print(\"테스트 데이터 전처리 중...\")\n",
        "test_df['document'] = test_df['document'].astype(str)\n",
        "test_df = test_df.dropna(subset=['document'])\n",
        "test_texts = test_df['document'].apply(preprocess_text)\n",
        "test_labels = test_df['label'].values\n",
        "\n",
        "# 빈 리뷰 제거\n",
        "train_mask = train_texts.apply(lambda x: len(x) > 0)\n",
        "train_texts = train_texts[train_mask]\n",
        "train_labels = train_labels[train_mask]\n",
        "\n",
        "test_mask = test_texts.apply(lambda x: len(x) > 0)\n",
        "test_texts = test_texts[test_mask]\n",
        "test_labels = test_labels[test_mask]\n",
        "\n",
        "print(f\"\\n전처리 후 학습 데이터: {len(train_texts)}개\")\n",
        "print(f\"전처리 후 테스트 데이터: {len(test_texts)}개\")\n",
        "print(f\"샘플 전처리 결과: {train_texts.iloc[0]}\")"
      ],
      "metadata": {
        "id": "QSAkHdLWe8gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 5단계: 토큰화 및 패딩\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"토큰화 및 시퀀스 변환...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 개선: vocab_size 증가, OOV 토큰 사용\n",
        "VOCAB_SIZE = 20000  # 15000 -> 20000으로 증가\n",
        "MAX_LENGTH = 80      # 적절한 길이로 조정\n",
        "\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "print(f\"전체 단어 수: {len(tokenizer.word_index)}\")\n",
        "print(f\"사용할 단어 수: {VOCAB_SIZE}\")\n",
        "\n",
        "# 시퀀스 변환\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "\n",
        "# 패딩\n",
        "train_padded = pad_sequences(train_sequences, maxlen=MAX_LENGTH,\n",
        "                             padding='post', truncating='post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=MAX_LENGTH,\n",
        "                           padding='post', truncating='post')\n",
        "\n",
        "print(f\"패딩 후 shape: {train_padded.shape}\")"
      ],
      "metadata": {
        "id": "ACYeqPTrfNEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 6단계: 개선된 모델 구축\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"모델 구축...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def create_improved_model():\n",
        "    \"\"\"개선된 양방향 LSTM 모델\"\"\"\n",
        "    model = Sequential([\n",
        "        # 임베딩 레이어 - 차원 증가\n",
        "        Embedding(VOCAB_SIZE, 128, input_length=MAX_LENGTH),\n",
        "\n",
        "        # 첫 번째 양방향 LSTM - return_sequences=True로 다층 구조\n",
        "        Bidirectional(LSTM(64, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)),\n",
        "\n",
        "        # 두 번째 양방향 LSTM\n",
        "        Bidirectional(LSTM(32, dropout=0.3, recurrent_dropout=0.3)),\n",
        "\n",
        "        # Dense 레이어\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        # 출력 레이어\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # Adam optimizer with custom learning rate\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "    model.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        optimizer=optimizer,\n",
        "        metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "model = create_improved_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "smSQrNlxfOG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 7단계: 콜백 설정 및 학습\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"모델 학습 시작...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 콜백 설정\n",
        "checkpoint_path = 'best_sentiment_model.weights.h5'\n",
        "\n",
        "callbacks = [\n",
        "    # 최고 성능 모델 저장\n",
        "    ModelCheckpoint(\n",
        "        checkpoint_path,\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "        mode='max',\n",
        "        verbose=1\n",
        "    ),\n",
        "\n",
        "    # 조기 종료\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=3,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "\n",
        "    # 학습률 감소\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=2,\n",
        "        min_lr=0.00001,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "# 모델 학습\n",
        "history = model.fit(\n",
        "    train_padded,\n",
        "    train_labels,\n",
        "    validation_split=0.2,  # 학습 데이터의 20%를 검증용으로 사용\n",
        "    batch_size=128,         # 배치 크기 증가\n",
        "    epochs=15,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "ZsmsZHR6fRJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 8단계: 학습 결과 시각화\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"학습 결과 시각화...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def plot_training_history(history):\n",
        "    \"\"\"학습 이력 시각화\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # 정확도 그래프\n",
        "    axes[0].plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    axes[0].set_title('Model Accuracy', fontsize=14)\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Accuracy')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True)\n",
        "\n",
        "    # 손실 그래프\n",
        "    axes[1].plot(history.history['loss'], label='Train Loss')\n",
        "    axes[1].plot(history.history['val_loss'], label='Validation Loss')\n",
        "    axes[1].set_title('Model Loss', fontsize=14)\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('Loss')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "plot_training_history(history)"
      ],
      "metadata": {
        "id": "HPMvtzZmfUFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 9단계: 테스트 데이터 평가\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"테스트 데이터 평가...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 최고 성능 가중치 로드\n",
        "model.load_weights(checkpoint_path)\n",
        "\n",
        "# 평가\n",
        "test_loss, test_acc, test_precision, test_recall = model.evaluate(\n",
        "    test_padded,\n",
        "    test_labels,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(f\"\\n최종 테스트 결과:\")\n",
        "print(f\"정확도: {test_acc:.4f}\")\n",
        "print(f\"정밀도: {test_precision:.4f}\")\n",
        "print(f\"재현율: {test_recall:.4f}\")\n",
        "print(f\"F1-Score: {2 * (test_precision * test_recall) / (test_precision + test_recall):.4f}\")"
      ],
      "metadata": {
        "id": "qAkdF94hfWxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 10단계: 예측 함수\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"예측 함수 준비 완료\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def predict_sentiment(text, show_detail=True):\n",
        "    \"\"\"\n",
        "    새로운 텍스트의 감정을 예측하는 함수\n",
        "\n",
        "    Args:\n",
        "        text (str): 예측할 텍스트\n",
        "        show_detail (bool): 상세 정보 출력 여부\n",
        "\n",
        "    Returns:\n",
        "        tuple: (예측 확률, 감정 레이블)\n",
        "    \"\"\"\n",
        "    # 전처리\n",
        "    processed = preprocess_text(text)\n",
        "\n",
        "    if len(processed) == 0:\n",
        "        return 0.5, \"중립\"\n",
        "\n",
        "    # 시퀀스 변환 및 패딩\n",
        "    sequence = tokenizer.texts_to_sequences([processed])\n",
        "    padded = pad_sequences(sequence, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
        "\n",
        "    # 예측\n",
        "    prediction = model.predict(padded, verbose=0)[0][0]\n",
        "\n",
        "    # 결과 해석\n",
        "    sentiment = \"긍정\" if prediction > 0.5 else \"부정\"\n",
        "    confidence = prediction if prediction > 0.5 else (1 - prediction)\n",
        "\n",
        "    if show_detail:\n",
        "        print(f\"\\n입력 텍스트: {text}\")\n",
        "        print(f\"전처리 결과: {processed}\")\n",
        "        print(f\"예측 점수: {prediction:.4f}\")\n",
        "        print(f\"감정: {sentiment} ({confidence*100:.2f}% 확신)\")\n",
        "\n",
        "    return prediction, sentiment"
      ],
      "metadata": {
        "id": "9hFaE0X8fb3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 11단계: 예측 테스트\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"예측 테스트\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 테스트 샘플\n",
        "test_samples = [\n",
        "    \"이 영화 진짜 최고였어요! 강추합니다!\",\n",
        "    \"완전 시간 낭비 ㅉㅉ 돈 아까워\",\n",
        "    \"와 개쩐다 정말 세계관 최강자들의 영화다\",\n",
        "    \"이게 영화야? 차라리 유튜브 보는게 나음\",\n",
        "    \"배우들 연기가 너무 좋았고 스토리도 탄탄했어요\",\n",
        "    \"졸려서 중간에 잤음 ㅋㅋ\",\n",
        "    \"기대 안했는데 의외로 괜찮네요\",\n",
        "    \"역대급 쓰레기 영화\"\n",
        "]\n",
        "\n",
        "for sample in test_samples:\n",
        "    predict_sentiment(sample)\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "X2HnaRa5fhkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 12단계: 모델 저장\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"모델 저장...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 전체 모델 저장\n",
        "model.save('sentiment_model_full.h5')\n",
        "print(\"모델이 'sentiment_model_full.h5'에 저장되었습니다.\")\n",
        "\n",
        "# 토크나이저 저장 (pickle 사용)\n",
        "import pickle\n",
        "with open('tokenizer.pickle', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "print(\"토크나이저가 'tokenizer.pickle'에 저장되었습니다.\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"모든 작업이 완료되었습니다!\")\n",
        "print(\"=\" * 50)\n"
      ],
      "metadata": {
        "id": "2iQrWg1Jfkpn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}